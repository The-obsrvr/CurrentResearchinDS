activation,relu
alpha,0.008119109390266386
batch_size,146
beta_1,0.9
beta_2,0.999
early_stopping,False
epsilon,1e-08
hidden_layer_sizes,[224  96]
learning_rate,constant
learning_rate_init,0.0003723540690828018
max_fun,15000
max_iter,1000
momentum,0.9
n_iter_no_change,10
nesterovs_momentum,True
power_t,0.5
random_state,42
shuffle,True
solver,adam
tol,0.0001
validation_fraction,0.1
verbose,False
warm_start,False
